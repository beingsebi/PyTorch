# [01. PyTorch Workflow Fundamentals](https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb)

* [learnpytorch.io link](https://www.learnpytorch.io/01_pytorch_workflow/)
* [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html)

---

The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discovered patterns to predict the future.

<img  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png"  width=900  alt="a pytorch workflow flowchat"/> 

Specifically, we're going to cover:

 
<table>
<thead>
<tr>
  <th><strong>Topic</strong></th>
  <th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>1. Getting data ready</strong></td>
  <td>Data can be almost anything but to get started we're going to create a simple straight line</td>
</tr>
<tr>
  <td><strong>2. Building a model</strong></td>
  <td>Here we'll create a model to learn patterns in the data, we'll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong>.</td>
</tr>
<tr>
  <td><strong>3. Fitting the model to data (training)</strong></td>
  <td>We've got data and a model, now let's let the model (try to) find patterns in the (<strong>training</strong>) data.</td>
</tr>
<tr>
  <td><strong>4. Making predictions and evaluating a model (inference)</strong></td>
  <td>Our model's found patterns in the data, let's compare its findings to the actual (<strong>testing</strong>) data.</td>
</tr>
<tr>
  <td><strong>5. Saving and loading a model</strong></td>
  <td>You may want to use your model elsewhere, or come back to it later, here we'll cover that.</td>
</tr>
<tr>
  <td><strong>6. Putting it all together</strong></td>
  <td>Let's take all of the above and combine it.</td>
</tr>
</tbody>
</table>

---
Again, it's best to learn via examples. Let's build a basic model that learns the pattern of a straight line and matches it. The model learns by *studying* `(x,y)` pairs, and after that, for given `x` values, it return corresponding `y` values.

## 0. Prerequisites
```py
import torch
from torch import nn # nn contains all of PyTorch's building blocks for neural networks
import matplotlib.pyplot as plt # conda install -c conda-forge matplotlib
```

## 1. Data preparing and loading
```py
# Create *known* parameters
weight = 0.7
bias = 0.3

# Create data
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias
X[:5], y[:5]
```

```cpp
(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560]]))
```

---

###  Splitting data into training and test sets

One of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).

Each split of the dataset serves a specific purpose:

<table>
<thead>
<tr>
  <th>Split</th>
  <th>Purpose</th>
  <th>Amount of total data</th>
  <th>How often is it used?</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Training set</strong></td>
  <td>The model learns from this data (like the course materials you study during the semester).</td>
  <td>~60-80%</td>
  <td>Always</td>
</tr>
<tr>
  <td><strong>Validation set</strong></td>
  <td>The model gets tuned on this data (like the practice exam you take before the final exam).</td>
  <td>~10-20%</td>
  <td>Often but not always</td>
</tr>
<tr>
  <td><strong>Testing set</strong></td>
  <td>The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester).</td>
  <td>~10-20%</td>
  <td>Always</td>
</tr>
</tbody>
</table>

For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on. We can create them by splitting our `X` and `y` tensors.
```py
# Create train/test split
train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing 
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)
```

```cpp
(40, 40, 10, 10)
```

The model we create is going to try and learn the relationship between `X_train` and `y_train` and then we will evaluate what it learns on `X_test` and `y_test`.

But right now our data is just numbers on a page. Let's create a function to visualize it.
```py
def plot_predictions(train_data=X_train,train_labels=y_train,test_data=X_test,test_labels=y_test,predictions=None):
  plt.figure(figsize=(10, 7))
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data") # Plot training data in blue
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data") # Plot test data in green
  if predictions is not None:
    # Plot the predictions in red (predictions were made on the test data)
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  plt.legend(prop={"size": 14}); # Show the legend
plot_predictions()
```
<img  src="https://i.imgur.com/TMpy5S3.png" alt="data visualisation"/> 

## 2. Build a model

Now we've got some data, let's build a model to use the blue dots to predict the green dots.
```py
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)
    def __init__(self):
        super().__init__() 
        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)
				                                dtype=torch.float), # <- PyTorch loves float32 by default
                                   requires_grad=True) # <- can we update this value with gradient descent?)
        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)
                                            dtype=torch.float), # <- PyTorch loves float32 by default
                                requires_grad=True) # <- can we update this value with gradient descent?))
    # Forward defines the computation in the model
    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- "x" is the input data (e.g. training/testing features)
        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)
```
PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine. They are  [`torch.nn`](https://pytorch.org/docs/stable/nn.html),  [`torch.optim`](https://pytorch.org/docs/stable/optim.html),  [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  and  [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).

<table>
<thead>
<tr>
  <th>PyTorch module</th>
  <th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr>
  <td><a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a></td>
  <td>Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter"><code>torch.nn.Parameter</code></a></td>
  <td>Stores tensors that can be used with <code>nn.Module</code>. If <code>requires_grad=True</code> gradients (used for updating model parameters via <a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"><strong>gradient descent</strong></a>)  are calculated automatically, this is often referred to as "autograd".</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>torch.nn.Module</code></a></td>
  <td>The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass <code>nn.Module</code>. Requires a <code>forward()</code> method be implemented.</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a></td>
  <td>Contains various optimization algorithms (these tell the model parameters stored in <code>nn.Parameter</code> how to best change to improve gradient descent and in turn reduce the loss).</td>
</tr>
<tr>
  <td><code>def forward()</code></td>
  <td>All <code>nn.Module</code> subclasses require a <code>forward()</code> method, this defines the computation that will take place on the data passed to the particular <code>nn.Module</code> (e.g. the linear regression formula above).</td>
</tr>
</tbody>
</table>

If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from `torch.nn`:
<ul>
<li><code>nn.Module</code> contains the larger building blocks (layers)</li>
<li><code>nn.Parameter</code> contains the smaller parameters like weights and biases (put these together to make <code>nn.Module</code>(s))</li>
<li><code>forward()</code> tells the larger blocks how to make calculations on inputs (tensors full of data) within  <code>nn.Module</code>(s)</li>
<li><code>torch.optim</code> contains optimization methods on how to improve the parameters within <code>nn.Parameter</code> to better represent input data</li>
</ul>

<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-model-annotated.png" alt="a pytorch linear model with annotations">

###  Checking the contents of a PyTorch model

Let's create a model instance with the class we've made and check its parameters using  [`.parameters()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters).

```py
# Set manual seed since nn.Parameter are randomly initialzied
torch.manual_seed(42)
# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))
model_0 = LinearRegressionModel()
# Check the nn.Parameter(s) within the nn.Module subclass we created
list(model_0.parameters())
```
```cpp
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]
```
  
We can also get the state (what the model contains) of the model using [`.state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict).

```py
model_0.state_dict()
```

```cpp
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])
```

### Making predictions using  `torch.inference_mode()`

We can pass the model the test data  `X_test`  to see how closely it predicts  `y_test`. When we pass data to our model, it'll go through the model's  `forward()`  method and produce a result using the computation we've defined. Let's make some predictions. `torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make *forward-passes* (data going through the `forward()` method) faster.

```py
with torch.inference_mode(): 
    y_preds = model_0(X_test)
    print(y_preds)
```
```cpp
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]])
```
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjc3NjM4OTI1LC0xMjUzOTQ2OTE5LDMyNz
M1ODEyNSwtNjkxMTAxNjc4LDY2NzY2NTk4OCwxNTYwMTg1NjM1
LDE4NjIzMjY3NzAsNzY3ODkyNjQsLTIxMTE2NjM0MDAsMTY2NT
M5NDMxOCwxNTYxMjA2NzQxLC05NTIzMzQzMiwtMTY4MTExNzQ4
OCw3MzA5OTgxMTZdfQ==
-->