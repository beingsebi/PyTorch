# [01. PyTorch Workflow Fundamentals](https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb)

* [learnpytorch.io link](https://www.learnpytorch.io/01_pytorch_workflow/)
* [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html)

---

The essence of machine learning and deep learning is to take some data from the past, build an algorithm (like a neural network) to discover patterns in it and use the discovered patterns to predict the future.

<img  src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01_a_pytorch_workflow.png"  width=900  alt="a pytorch workflow flowchat"/> 

Specifically, we're going to cover:

 
<table>
<thead>
<tr>
  <th><strong>Topic</strong></th>
  <th><strong>Contents</strong></th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>1. Getting data ready</strong></td>
  <td>Data can be almost anything but to get started we're going to create a simple straight line</td>
</tr>
<tr>
  <td><strong>2. Building a model</strong></td>
  <td>Here we'll create a model to learn patterns in the data, we'll also choose a <strong>loss function</strong>, <strong>optimizer</strong> and build a <strong>training loop</strong>.</td>
</tr>
<tr>
  <td><strong>3. Fitting the model to data (training)</strong></td>
  <td>We've got data and a model, now let's let the model (try to) find patterns in the (<strong>training</strong>) data.</td>
</tr>
<tr>
  <td><strong>4. Making predictions and evaluating a model (inference)</strong></td>
  <td>Our model's found patterns in the data, let's compare its findings to the actual (<strong>testing</strong>) data.</td>
</tr>
<tr>
  <td><strong>5. Saving and loading a model</strong></td>
  <td>You may want to use your model elsewhere, or come back to it later, here we'll cover that.</td>
</tr>
<tr>
  <td><strong>6. Putting it all together</strong></td>
  <td>Let's take all of the above and combine it.</td>
</tr>
</tbody>
</table>

---
Again, it's best to learn via examples. Let's build a basic model that learns the pattern of a straight line and matches it. The model learns by *studying* `(x,y)` pairs, and after that, for given `x` values, it return corresponding `y` values.

## 0. Prerequisites
```py
import torch
from torch import nn # nn contains all of PyTorch's building blocks for neural networks
import matplotlib.pyplot as plt # conda install -c conda-forge matplotlib
```

## 1. Data preparing and loading
```py
# Create *known* parameters
weight = 0.7
bias = 0.3

# Create data
start = 0
end = 1
step = 0.02
X = torch.arange(start, end, step).unsqueeze(dim=1)
y = weight * X + bias
X[:5], y[:5]
```

```cpp
(tensor([[0.0000],
         [0.0200],
         [0.0400],
         [0.0600],
         [0.0800]]),
 tensor([[0.3000],
         [0.3140],
         [0.3280],
         [0.3420],
         [0.3560]]))
```

---

###  Splitting data into training and test sets

One of most important steps in a machine learning project is creating a training and test set (and when required, a validation set).

Each split of the dataset serves a specific purpose:

<table>
<thead>
<tr>
  <th>Split</th>
  <th>Purpose</th>
  <th>Amount of total data</th>
  <th>How often is it used?</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Training set</strong></td>
  <td>The model learns from this data (like the course materials you study during the semester).</td>
  <td>~60-80%</td>
  <td>Always</td>
</tr>
<tr>
  <td><strong>Validation set</strong></td>
  <td>The model gets tuned on this data (like the practice exam you take before the final exam).</td>
  <td>~10-20%</td>
  <td>Often but not always</td>
</tr>
<tr>
  <td><strong>Testing set</strong></td>
  <td>The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester).</td>
  <td>~10-20%</td>
  <td>Always</td>
</tr>
</tbody>
</table>

For now, we'll just use a training and test set, this means we'll have a dataset for our model to learn on as well as be evaluated on. We can create them by splitting our `X` and `y` tensors.
```py
# Create train/test split
train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing 
X_train, y_train = X[:train_split], y[:train_split]
X_test, y_test = X[train_split:], y[train_split:]

len(X_train), len(y_train), len(X_test), len(y_test)
```

```cpp
(40, 40, 10, 10)
```

The model we create is going to try and learn the relationship between `X_train` and `y_train` and then we will evaluate what it learns on `X_test` and `y_test`.

But right now our data is just numbers on a page. Let's create a function to visualize it.
```py
def plot_predictions(train_data=X_train,train_labels=y_train,test_data=X_test,test_labels=y_test,predictions=None):
  plt.figure(figsize=(10, 7))
  plt.scatter(train_data, train_labels, c="b", s=4, label="Training data") # Plot training data in blue
  plt.scatter(test_data, test_labels, c="g", s=4, label="Testing data") # Plot test data in green
  if predictions is not None:
    # Plot the predictions in red (predictions were made on the test data)
    plt.scatter(test_data, predictions, c="r", s=4, label="Predictions")
  plt.legend(prop={"size": 14}); # Show the legend
plot_predictions()
```
<img  src="https://i.imgur.com/TMpy5S3.png" alt="data visualisation"/> 

## 2. Build a model

Now we've got some data, let's build a model to use the blue dots to predict the green dots.
```py
class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)
    def __init__(self):
        super().__init__() 
        self.weights = nn.Parameter(torch.randn(1, # <- start with random weights (this will get adjusted as the model learns)
				                                dtype=torch.float), # <- PyTorch loves float32 by default
                                   requires_grad=True) # <- can we update this value with gradient descent?)
        self.bias = nn.Parameter(torch.randn(1, # <- start with random bias (this will get adjusted as the model learns)
                                            dtype=torch.float), # <- PyTorch loves float32 by default
                                requires_grad=True) # <- can we update this value with gradient descent?))
    # Forward defines the computation in the model
    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- "x" is the input data (e.g. training/testing features)
        return self.weights * x + self.bias # <- this is the linear regression formula (y = m*x + b)
```
PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine. They are  [`torch.nn`](https://pytorch.org/docs/stable/nn.html),  [`torch.optim`](https://pytorch.org/docs/stable/optim.html),  [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  and  [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html). For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).

<table>
<thead>
<tr>
  <th>PyTorch module</th>
  <th>What does it do?</th>
</tr>
</thead>
<tbody>
<tr>
  <td><a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a></td>
  <td>Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#parameter"><code>torch.nn.Parameter</code></a></td>
  <td>Stores tensors that can be used with <code>nn.Module</code>. If <code>requires_grad=True</code> gradients (used for updating model parameters via <a href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html"><strong>gradient descent</strong></a>)  are calculated automatically, this is often referred to as "autograd".</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module"><code>torch.nn.Module</code></a></td>
  <td>The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass <code>nn.Module</code>. Requires a <code>forward()</code> method be implemented.</td>
</tr>
<tr>
  <td><a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a></td>
  <td>Contains various optimization algorithms (these tell the model parameters stored in <code>nn.Parameter</code> how to best change to improve gradient descent and in turn reduce the loss).</td>
</tr>
<tr>
  <td><code>def forward()</code></td>
  <td>All <code>nn.Module</code> subclasses require a <code>forward()</code> method, this defines the computation that will take place on the data passed to the particular <code>nn.Module</code> (e.g. the linear regression formula above).</td>
</tr>
</tbody>
</table>

If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from `torch.nn`:
<ul>
<li><code>nn.Module</code> contains the larger building blocks (layers)</li>
<li><code>nn.Parameter</code> contains the smaller parameters like weights and biases (put these together to make <code>nn.Module</code>(s))</li>
<li><code>forward()</code> tells the larger blocks how to make calculations on inputs (tensors full of data) within  <code>nn.Module</code>(s)</li>
<li><code>torch.optim</code> contains optimization methods on how to improve the parameters within <code>nn.Parameter</code> to better represent input data</li>
</ul>

<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-linear-model-annotated.png" alt="a pytorch linear model with annotations">

###  Checking the contents of a PyTorch model

Let's create a model instance with the class we've made and check its parameters using  [`.parameters()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.parameters).

```py
# Set manual seed since nn.Parameter are randomly initialzied
torch.manual_seed(42)
# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))
model_0 = LinearRegressionModel()
# Check the nn.Parameter(s) within the nn.Module subclass we created
list(model_0.parameters())
```
```cpp
[Parameter containing:
 tensor([0.3367], requires_grad=True),
 Parameter containing:
 tensor([0.1288], requires_grad=True)]
```
  
We can also get the state (what the model contains) of the model using [`.state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict).

```py
model_0.state_dict()
```

```cpp
OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])
```

### Making predictions using  `torch.inference_mode()`

We can pass the model the test data  `X_test`  to see how closely it predicts  `y_test`. When we pass data to our model, it'll go through the model's  `forward()`  method and produce a result using the computation we've defined. Let's make some predictions. `torch.inference_mode()` turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make *forward-passes* (data going through the `forward()` method) faster.

```py
with torch.inference_mode(): 
    y_preds = model_0(X_test)
    print(y_preds)
```
```cpp
tensor([[0.3982],
        [0.4049],
        [0.4116],
        [0.4184],
        [0.4251],
        [0.4318],
        [0.4386],
        [0.4453],
        [0.4520],
        [0.4588]])
```
Let's **visualize** the data.
```py
plot_predictions(predictions=y_preds)
```
<img src="https://i.imgur.com/waKJgv7.png">
As you can see, our model is pretty dumb yet. It hasn't even looked at the blue dots to try to learn a pattern. Time to change that.

  
## 3. Train the model
Right now our model is making predictions using random parameters to make calculations, it's basically guessing (randomly).

To fix that, we can update its internal parameters (I also refer to *parameters* as patterns), the `weights` and `bias` values we set randomly using `nn.Parameter()` and `torch.randn()` to be something that better represents the data. We could hard code this (since we know the default values `weight=0.7` and `bias=0.3`) but where's the fun in that? Most of the time you won't know what the ideal parameters are for a model. Instead, it's much more fun to write code to see if the model can try and figure them out itself.

### Creating a loss function and optimizer in PyTorch

For our model to update its parameters on its own, we'll need to add a few more things to our recipe. That's a **loss function** and an **optimizer**.

<table>
<thead>
<tr>
  <th>Function</th>
  <th>What does it do?</th>
  <th>Where does it live in PyTorch?</th>
  <th>Common values</th>
</tr>
</thead>
<tbody>
<tr>
  <td><strong>Loss function</strong></td>
  <td>Measures how wrong your models predictions (e.g. <code>y_preds</code>) are compared to the truth labels (e.g. <code>y_test</code>). Lower the better.</td>
  <td>PyTorch has plenty of built-in loss functions in <a href="https://pytorch.org/docs/stable/nn.html#loss-functions"><code>torch.nn</code></a>.</td>
  <td>Mean absolute error (MAE) for regression problems (<a href="https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html"><code>torch.nn.L1Loss()</code></a>). Binary cross entropy for binary classification problems (<a href="https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html"><code>torch.nn.BCELoss()</code></a>).</td>
</tr>
<tr>
  <td><strong>Optimizer</strong></td>
  <td>Tells your model how to update its internal parameters to best lower the loss.</td>
  <td>You can find various optimization function implementations in <a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>.</td>
  <td>Stochastic gradient descent (<a href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD"><code>torch.optim.SGD()</code></a>). Adam optimizer (<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam"><code>torch.optim.Adam()</code></a>).</td>
</tr>
</tbody>
</table>

Let's see it in code:
```py
# Create the loss function
loss_fn = nn.L1Loss() # MAE loss is same as L1Loss

# Create the optimizer
optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize
                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))
```

### Creating an optimization loop in PyTorch

Now that we've got a loss function and an optimizer, it's now time to create a  **training loop**  (and  **testing loop**).

The training loop involves the model going through the training data and learning the relationships between the  `features`  and  `labels`.

The testing loop involves going through the testing data and evaluating how good the patterns are that the model learned on the training data (the model never see's the testing data during training).

Each of these is called a "loop" because we want our model to look (loop through) at each sample in each dataset.

For the **training loop**, we'll build the following steps:

<table>
<thead>
<tr>
  <th>Number</th>
  <th>Step name</th>
  <th>What does it do?</th>
  <th>Code example</th>
</tr>
</thead>
<tbody>
<tr>
  <td>1</td>
  <td>Forward pass</td>
  <td>The model goes through all of the training data once, performing its <code>forward()</code> function calculations.</td>
  <td><code>model(x_train)</code></td>
</tr>
<tr>
  <td>2</td>
  <td>Calculate the loss</td>
  <td>The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are.</td>
  <td><code>loss = loss_fn(y_pred, y_train)</code></td>
</tr>
<tr>
  <td>3</td>
  <td>Zero gradients</td>
  <td>The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step.</td>
  <td><code>optimizer.zero_grad()</code></td>
</tr>
<tr>
  <td>4</td>
  <td>Perform backpropagation on the loss</td>
  <td>Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with <code>requires_grad=True</code>). This is known as <strong>backpropagation</strong>, hence "backwards".</td>
  <td><code>loss.backward()</code></td>
</tr>
<tr>
  <td>5</td>
  <td>Update the optimizer (<strong>gradient descent</strong>)</td>
  <td>Update the parameters with <code>requires_grad=True</code> with respect to the loss gradients in order to improve them.</td>
  <td><code>optimizer.step()</code></td>
</tr>
</tbody>
</table>
<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/01-pytorch-training-loop-annotated.png" alt="pytorch training loop annotated">
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTc5MDM1MTQzNywtMTQ4MTI0Mzg1NCwtNz
c0ODgyOTE0LC0xMjUzOTQ2OTE5LDMyNzM1ODEyNSwtNjkxMTAx
Njc4LDY2NzY2NTk4OCwxNTYwMTg1NjM1LDE4NjIzMjY3NzAsNz
Y3ODkyNjQsLTIxMTE2NjM0MDAsMTY2NTM5NDMxOCwxNTYxMjA2
NzQxLC05NTIzMzQzMiwtMTY4MTExNzQ4OCw3MzA5OTgxMTZdfQ
==
-->